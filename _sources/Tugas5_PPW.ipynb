{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b4e13424",
      "metadata": {},
      "source": [
        "# TF-IDF & Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569646e0",
      "metadata": {},
      "source": [
        "## TF - IDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "hxFU_xG68tuI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxFU_xG68tuI",
        "outputId": "78c49331-e76a-47e2-e41f-b768cdacef1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Berhasil memuat seluruh data dari 'berita_tempo.csv'.\n",
            "---\n",
            ">>> Count Vectorizer (Matriks Term Frequency):\n",
            "     nan\n",
            "0      1\n",
            "1      1\n",
            "2      1\n",
            "3      1\n",
            "4      1\n",
            "5      1\n",
            "6      1\n",
            "7      1\n",
            "8      1\n",
            "9      1\n",
            "10     1\n",
            "11     1\n",
            "12     1\n",
            "13     1\n",
            "14     1\n",
            "15     1\n",
            "16     1\n",
            "17     1\n",
            "18     1\n",
            "19     1\n",
            "20     1\n",
            "21     1\n",
            "22     1\n",
            "23     1\n",
            "24     1\n",
            "25     1\n",
            "26     1\n",
            "27     1\n",
            "28     1\n",
            "29     1\n",
            "30     1\n",
            "31     1\n",
            "32     1\n",
            "33     1\n",
            "34     1\n",
            "35     1\n",
            "36     1\n",
            "37     1\n",
            "38     1\n",
            "39     1\n",
            "40     1\n",
            "41     1\n",
            "42     1\n",
            "43     1\n",
            "44     1\n",
            "45     1\n",
            "46     1\n",
            "47     1\n",
            "48     1\n",
            "49     1\n",
            "50     1\n",
            "51     1\n",
            "52     1\n",
            "53     1\n",
            "54     1\n",
            "55     1\n",
            "56     1\n",
            "57     1\n",
            "58     1\n",
            "59     1\n",
            "60     1\n",
            "61     1\n",
            "62     1\n",
            "63     1\n",
            "64     1\n",
            "65     1\n",
            "66     1\n",
            "67     1\n",
            "68     1\n",
            "69     1\n",
            "70     1\n",
            "71     1\n",
            "72     1\n",
            "73     1\n",
            "74     1\n",
            "75     1\n",
            "76     1\n",
            "77     1\n",
            "78     1\n",
            "79     1\n",
            "80     1\n",
            "81     1\n",
            "82     1\n",
            "83     1\n",
            "84     1\n",
            "85     1\n",
            "86     1\n",
            "87     1\n",
            "88     1\n",
            "89     1\n",
            "90     1\n",
            "91     1\n",
            "92     1\n",
            "93     1\n",
            "94     1\n",
            "95     1\n",
            "96     1\n",
            "97     1\n",
            "98     1\n",
            "99     1\n",
            "100    1\n",
            "101    1\n",
            "102    1\n",
            "103    1\n",
            "104    1\n",
            "105    1\n",
            "106    1\n",
            "107    1\n",
            "108    1\n",
            "109    1\n",
            "110    1\n",
            "111    1\n",
            "112    1\n",
            "113    1\n",
            "114    1\n",
            "115    1\n",
            "116    1\n",
            "117    1\n",
            "118    1\n",
            "119    1\n",
            "120    1\n",
            "121    1\n",
            "122    1\n",
            "123    1\n",
            "124    1\n",
            "125    1\n",
            "126    1\n",
            "127    1\n",
            "128    1\n",
            "129    1\n",
            "130    1\n",
            "131    1\n",
            "132    1\n",
            "133    1\n",
            "134    1\n",
            "135    1\n",
            "136    1\n",
            "137    1\n",
            "138    1\n",
            "139    1\n",
            "140    1\n",
            "141    1\n",
            "142    1\n",
            "143    1\n",
            "144    1\n",
            "145    1\n",
            "146    1\n",
            "147    1\n",
            "148    1\n",
            "--------------------------------------------------\n",
            ">>> IDF (Inverse Document Frequency):\n",
            "  word  idf_value\n",
            "0  nan        1.0\n",
            "--------------------------------------------------\n",
            ">>> TF-IDF (Term Frequency - Inverse Document Frequency):\n",
            "     nan\n",
            "0    1.0\n",
            "1    1.0\n",
            "2    1.0\n",
            "3    1.0\n",
            "4    1.0\n",
            "5    1.0\n",
            "6    1.0\n",
            "7    1.0\n",
            "8    1.0\n",
            "9    1.0\n",
            "10   1.0\n",
            "11   1.0\n",
            "12   1.0\n",
            "13   1.0\n",
            "14   1.0\n",
            "15   1.0\n",
            "16   1.0\n",
            "17   1.0\n",
            "18   1.0\n",
            "19   1.0\n",
            "20   1.0\n",
            "21   1.0\n",
            "22   1.0\n",
            "23   1.0\n",
            "24   1.0\n",
            "25   1.0\n",
            "26   1.0\n",
            "27   1.0\n",
            "28   1.0\n",
            "29   1.0\n",
            "30   1.0\n",
            "31   1.0\n",
            "32   1.0\n",
            "33   1.0\n",
            "34   1.0\n",
            "35   1.0\n",
            "36   1.0\n",
            "37   1.0\n",
            "38   1.0\n",
            "39   1.0\n",
            "40   1.0\n",
            "41   1.0\n",
            "42   1.0\n",
            "43   1.0\n",
            "44   1.0\n",
            "45   1.0\n",
            "46   1.0\n",
            "47   1.0\n",
            "48   1.0\n",
            "49   1.0\n",
            "50   1.0\n",
            "51   1.0\n",
            "52   1.0\n",
            "53   1.0\n",
            "54   1.0\n",
            "55   1.0\n",
            "56   1.0\n",
            "57   1.0\n",
            "58   1.0\n",
            "59   1.0\n",
            "60   1.0\n",
            "61   1.0\n",
            "62   1.0\n",
            "63   1.0\n",
            "64   1.0\n",
            "65   1.0\n",
            "66   1.0\n",
            "67   1.0\n",
            "68   1.0\n",
            "69   1.0\n",
            "70   1.0\n",
            "71   1.0\n",
            "72   1.0\n",
            "73   1.0\n",
            "74   1.0\n",
            "75   1.0\n",
            "76   1.0\n",
            "77   1.0\n",
            "78   1.0\n",
            "79   1.0\n",
            "80   1.0\n",
            "81   1.0\n",
            "82   1.0\n",
            "83   1.0\n",
            "84   1.0\n",
            "85   1.0\n",
            "86   1.0\n",
            "87   1.0\n",
            "88   1.0\n",
            "89   1.0\n",
            "90   1.0\n",
            "91   1.0\n",
            "92   1.0\n",
            "93   1.0\n",
            "94   1.0\n",
            "95   1.0\n",
            "96   1.0\n",
            "97   1.0\n",
            "98   1.0\n",
            "99   1.0\n",
            "100  1.0\n",
            "101  1.0\n",
            "102  1.0\n",
            "103  1.0\n",
            "104  1.0\n",
            "105  1.0\n",
            "106  1.0\n",
            "107  1.0\n",
            "108  1.0\n",
            "109  1.0\n",
            "110  1.0\n",
            "111  1.0\n",
            "112  1.0\n",
            "113  1.0\n",
            "114  1.0\n",
            "115  1.0\n",
            "116  1.0\n",
            "117  1.0\n",
            "118  1.0\n",
            "119  1.0\n",
            "120  1.0\n",
            "121  1.0\n",
            "122  1.0\n",
            "123  1.0\n",
            "124  1.0\n",
            "125  1.0\n",
            "126  1.0\n",
            "127  1.0\n",
            "128  1.0\n",
            "129  1.0\n",
            "130  1.0\n",
            "131  1.0\n",
            "132  1.0\n",
            "133  1.0\n",
            "134  1.0\n",
            "135  1.0\n",
            "136  1.0\n",
            "137  1.0\n",
            "138  1.0\n",
            "139  1.0\n",
            "140  1.0\n",
            "141  1.0\n",
            "142  1.0\n",
            "143  1.0\n",
            "144  1.0\n",
            "145  1.0\n",
            "146  1.0\n",
            "147  1.0\n",
            "148  1.0\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "# Konfigurasi Pandas untuk tampilan penuh\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Fungsi untuk membersihkan teks\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Membersihkan teks dengan mengubahnya menjadi huruf kecil,\n",
        "    menghapus tanda baca, angka, dan spasi berlebih.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)   # hapus tanda baca\n",
        "    text = re.sub(r'\\d+', '', text)       # hapus angka\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # hapus spasi berlebih\n",
        "    return text\n",
        "\n",
        "# Menggunakan try-except untuk membaca data dari file CSV\n",
        "try:\n",
        "    # Membaca seluruh data dari file CSV berita_tempo.csv\n",
        "    df = pd.read_csv('berita_tempo.csv')\n",
        "    print(\"Berhasil memuat seluruh data dari 'berita_tempo.csv'.\")\n",
        "    \n",
        "    # Gunakan kolom 'isi' sebagai korpus\n",
        "    corpus = df['isi'].astype(str).tolist()\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File 'berita_tempo.csv' tidak ditemukan.\")\n",
        "    print(\"Menggunakan data dummy untuk demonstrasi.\")\n",
        "    corpus = [\n",
        "        \"Pelatih Atletico Madrid, Diego Simeone, mengaku kesal saat dihina fans Liverpool sepanjang pertandingan.\",\n",
        "        \"Atletico Madrid takluk dari Liverpool dengan skor 2-3 di Anfield pada lanjutan Liga Champions.\",\n",
        "        \"Bicara soal seberapa hebat Lionel Messi, Zinedine Zidane punya sudut pandang berbeda.\",\n",
        "        \"Marc Marquez menjalani operasi pada lengannya usai kecelakaan di MotoGP Portugal 2025.\",\n",
        "        \"Pebalap Yamaha, Alex Rins akan menggunakan motor M1 yang dikembangkan untuk 2026.\"\n",
        "    ]\n",
        "\n",
        "# Menerapkan pembersihan teks pada korpus\n",
        "cleaned_corpus = [clean_text(text) for text in corpus]\n",
        "\n",
        "# Inisialisasi CountVectorizer untuk menghitung Term Frequency (TF)\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(cleaned_corpus)\n",
        "count_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Inisialisasi TfidfTransformer untuk menghitung IDF dan TF-IDF\n",
        "transformer = TfidfTransformer()\n",
        "\n",
        "# ---\n",
        "### 1. Hasil Count Vectorizer (Matriks Term Frequency)\n",
        "print(\"---\")\n",
        "print(\">>> Count Vectorizer (Matriks Term Frequency):\")\n",
        "print(count_df.to_string())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# ---\n",
        "### 2. Hasil IDF (Inverse Document Frequency)\n",
        "transformer.fit(X)\n",
        "idf_values = transformer.idf_\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "idf_df = pd.DataFrame({'word': feature_names, 'idf_value': idf_values})\n",
        "print(\">>> IDF (Inverse Document Frequency):\")\n",
        "print(idf_df.to_string())\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# ---\n",
        "### 3. Hasil TF-IDF (Term Frequency - Inverse Document Frequency)\n",
        "tfidf_matrix = transformer.transform(X)\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "print(\">>> TF-IDF (Term Frequency - Inverse Document Frequency):\")\n",
        "print(tfidf_df.to_string())\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b22541e7",
      "metadata": {},
      "source": [
        "## Word Embeding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "nCVp2k6sWE87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCVp2k6sWE87",
        "outputId": "e3445cad-ba70-4dd2-b20c-c6013df030c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Bagian 1: Membuat Word Embedding dengan Gensim\n",
            "---\n",
            "Berhasil memuat seluruh data dari 'berita_tempo.csv'.\n",
            "\n",
            ">>> Word Embeddings yang sudah terlatih (contoh beberapa kata):\n",
            "                0         1         2         3         4         5         6         7         8         9         10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43        44        45        46        47        48        49        50        51        52        53        54        55        56        57        58        59        60        61        62        63        64        65        66        67        68        69        70        71        72        73        74        75        76        77        78        79        80        81        82        83        84        85        86        87        88        89        90        91        92        93        94        95        96        97        98        99        100       101       102       103       104       105       106       107       108       109       110       111       112       113       114       115       116       117       118       119       120       121       122       123       124       125       126       127\n",
            "di        -0.000034 -0.004503  0.004510  0.009194 -0.002643 -0.005972  0.006196  0.003979 -0.002230  0.000770  0.008108 -0.001315 -0.006881  0.000438  0.001864  0.006038 -0.002193  0.000571 -0.012555  0.000307  0.009450  0.006759 -0.000783 -0.006671 -0.002186  0.000674 -0.005726  0.006825 -0.007185 -0.005429 -0.007549 -0.001111  0.007993 -0.003636 -0.004418  0.003742  0.011086 -0.006574  0.001210 -0.001652 -0.009928  0.004943 -0.010532 -0.005316  0.003925 -0.000601 -0.008743  0.006476  0.002791  0.008124 -0.001933  0.005638 -0.003610  0.002033  0.006025 -0.003840  0.009937 -0.007726 -0.006496  0.006532 -0.004510  0.001547  0.000011 -0.005516  0.004958  0.001252  0.000216  0.006027 -0.002908 -0.002632  0.003818  0.003181 -0.008690 -0.012742  0.003390 -0.003059  0.005210 -0.003098 -0.004947 -0.003129 -0.003559  0.001841  0.007585  0.011971 -0.001214  0.007374  0.009448 -0.009583  0.001920  0.009159 -0.001778 -0.001600  0.004355 -0.002375  0.013823  0.006571 -0.010454 -0.009631  0.000052  0.004088 -0.013764  0.005296  0.002119  0.004577  0.007175 -0.002731 -0.002155  0.007681  0.000388 -0.009499  0.003869 -0.010747 -0.004692  0.006309  0.000721  0.007005  0.006522  0.007240 -0.000719  0.000910 -0.000684 -0.008565  0.004242 -0.006204  0.001981 -0.001579 -0.006919  0.003910\n",
            "indonesia -0.001119  0.003185  0.008082 -0.002392  0.001740 -0.004910  0.003875  0.000745  0.005973  0.006568  0.008788  0.007295  0.004965 -0.007403 -0.004428  0.002982 -0.004294  0.006482  0.001628  0.002510  0.002631  0.002875  0.003283 -0.011219 -0.003453  0.004126 -0.003166  0.007402  0.006684  0.004062 -0.001677  0.006042 -0.006644  0.003782 -0.005031 -0.001674  0.005282  0.003681  0.006322 -0.003713  0.005294  0.005642 -0.004189 -0.007772  0.005906  0.004997 -0.002310 -0.006264 -0.005947 -0.001609  0.006032 -0.001960 -0.007476  0.003909  0.003805 -0.005399  0.003834 -0.002530 -0.001543 -0.008122  0.001123 -0.003037  0.002417 -0.001110  0.004320 -0.006387 -0.001909  0.002824  0.003765 -0.003920 -0.007446  0.002182 -0.003627  0.000032 -0.004926 -0.002385  0.005952  0.004460 -0.004215  0.003626 -0.007715  0.004395 -0.002217  0.000498  0.008990  0.003203  0.008116 -0.007128  0.005851  0.009483 -0.007700 -0.004891 -0.005140 -0.004881 -0.000365 -0.005591  0.004682 -0.005986  0.006341  0.003408 -0.008072  0.004132  0.003790 -0.005646 -0.005191 -0.000850 -0.007876  0.000028  0.001018  0.000528  0.002992 -0.001839 -0.004190  0.001502 -0.001876  0.006160  0.003337  0.004525  0.002426 -0.001502 -0.004030 -0.002661  0.000317 -0.001059 -0.006852 -0.007563 -0.008620 -0.003965\n",
            "timnas    -0.000914 -0.005826 -0.004466 -0.001907  0.005749 -0.003782  0.007632 -0.000375 -0.004898  0.009261  0.007464  0.004152 -0.006816  0.002329  0.006492  0.007844  0.001070  0.001736 -0.005806  0.010820  0.009322  0.004544 -0.005372 -0.003955 -0.003021 -0.004912 -0.008976  0.001206  0.000745 -0.005617 -0.004906 -0.005515  0.006463  0.001657 -0.004992  0.007146  0.009963 -0.003714  0.006637  0.005149  0.003699  0.000950  0.004762 -0.006462 -0.004378  0.007341 -0.001721 -0.002723  0.003089 -0.002632  0.004593  0.006695  0.004617 -0.005004  0.007187  0.003457  0.006414 -0.005948  0.004942 -0.002343  0.005753 -0.003452 -0.004325  0.005891  0.004420 -0.001705  0.007692  0.008132 -0.008399 -0.000449  0.004890  0.001388 -0.002565 -0.002505  0.001149 -0.004336 -0.005795 -0.003468  0.001654  0.008601  0.006133 -0.005279 -0.005430  0.010693  0.004524  0.007088  0.007295 -0.004939  0.007822  0.004356  0.000835 -0.002981 -0.001704 -0.003095  0.006152  0.001715 -0.009295 -0.010157 -0.004979 -0.000832 -0.001809  0.008957  0.004021 -0.003187  0.001000  0.005323  0.004423  0.000501  0.004907 -0.006551  0.001350  0.004608 -0.001688 -0.007401  0.003260 -0.002063 -0.000888  0.008975 -0.000553 -0.003025 -0.004940  0.003099 -0.005570  0.002748 -0.005746  0.003076  0.001658  0.002662\n",
            "nepal     -0.000722 -0.005381  0.007499  0.005685  0.001316 -0.007712 -0.005112 -0.002801  0.000825  0.000428  0.009737 -0.000316  0.002582 -0.009206  0.002003  0.001047 -0.008679  0.000584 -0.009531  0.003118  0.005445  0.006744 -0.002873 -0.002942 -0.007693  0.007777 -0.008156  0.005632 -0.007553  0.000170 -0.004804 -0.003526 -0.001914  0.005396  0.002667 -0.000836  0.003890 -0.007973  0.007390  0.008579 -0.003909  0.001605 -0.001809  0.003291 -0.003946 -0.000261  0.004239  0.001157 -0.000113 -0.006766  0.009873 -0.003578 -0.002410  0.003975 -0.001148  0.001405  0.005504 -0.006461 -0.010242  0.007295  0.003038 -0.004504  0.004522  0.000009  0.007521 -0.006773  0.003761  0.004307  0.007144 -0.006272 -0.001095 -0.008240 -0.012792 -0.010801 -0.000976 -0.007804  0.003377 -0.006660  0.000213  0.003624 -0.004721 -0.001215  0.009833  0.011727 -0.001288  0.011336 -0.001783  0.000653  0.000504  0.005340 -0.000154  0.004284 -0.000932  0.005056  0.010052 -0.006042 -0.008003 -0.008438  0.001638 -0.002683 -0.010519 -0.004664  0.005328  0.001558 -0.006755 -0.002206  0.001104 -0.001631  0.005812 -0.006328  0.004854 -0.009266  0.004354  0.004647 -0.000752  0.002740 -0.005951  0.002727  0.000896 -0.006381 -0.009507 -0.004878  0.001451  0.003489 -0.007376 -0.008268 -0.006241 -0.001901\n",
            "dunia     -0.005054 -0.010418 -0.001786 -0.005768 -0.002070 -0.005018 -0.001277 -0.003150 -0.006308  0.001902  0.007995  0.005920 -0.007730 -0.005885  0.005948 -0.002856 -0.001645  0.005603  0.000799 -0.001889 -0.001393 -0.004690 -0.003704  0.003407  0.000963  0.006147  0.000709 -0.001173  0.007019 -0.006893 -0.007046 -0.001716 -0.000311 -0.001131 -0.001869  0.009084  0.002698 -0.001854  0.003028 -0.004993  0.004983  0.007327 -0.009562  0.000685  0.009927 -0.006582 -0.006862 -0.006853  0.005837  0.000158  0.009948 -0.005261  0.003060  0.003213  0.000335  0.001853 -0.001854 -0.008508 -0.000278  0.004428  0.004418  0.005384  0.001351  0.006851 -0.003841  0.002222  0.000895  0.003711  0.005192 -0.007867  0.004474 -0.009509 -0.004986 -0.010291 -0.003905  0.005482 -0.002077  0.000532  0.005616  0.009438 -0.008237 -0.003209  0.009694  0.008077  0.000713 -0.003144  0.004870 -0.008833  0.009058 -0.001305 -0.002810 -0.000238  0.000676 -0.000305  0.008873  0.001298 -0.004264 -0.003516  0.004346  0.000186 -0.004770  0.009073 -0.005031 -0.000722  0.008161  0.006943 -0.000206 -0.005366  0.002402 -0.004547  0.010054 -0.001274  0.000867  0.002558 -0.002866 -0.003072  0.000478 -0.000780  0.006386  0.008082 -0.001846 -0.000940 -0.006334  0.002003 -0.007427 -0.008988 -0.006546 -0.006522\n",
            "vs         0.006387 -0.006550  0.007370 -0.004537  0.004751  0.007033 -0.005234 -0.009103 -0.002745 -0.000598  0.001066 -0.006762 -0.007988  0.000589 -0.003697 -0.004190 -0.003593 -0.006715 -0.008406 -0.003081  0.001039 -0.001921  0.002754 -0.002369 -0.005982  0.006559 -0.006733 -0.002383 -0.005871 -0.004475 -0.002462  0.005376 -0.002162  0.005599  0.004277 -0.003761  0.006071  0.004246 -0.002048  0.005998  0.004316 -0.004873 -0.006595  0.001174  0.000400 -0.004634  0.005669 -0.004864 -0.005745  0.000320  0.000032  0.001330 -0.002962  0.000311 -0.000684  0.000243  0.009251 -0.005363 -0.002774  0.003820  0.005155 -0.002960  0.008825  0.006975 -0.000795 -0.000033  0.003417 -0.002388 -0.004974 -0.007167 -0.000628 -0.001513 -0.000341 -0.004237  0.000303  0.000985  0.003935 -0.001760 -0.006488 -0.006102 -0.000868  0.004802  0.002913 -0.004018 -0.004823  0.007828 -0.000653  0.002714 -0.003573  0.008591 -0.006136  0.001635 -0.000058 -0.006332 -0.000048  0.000500 -0.005912 -0.003427 -0.006091  0.001524  0.004596 -0.000986 -0.000751  0.003321  0.007660 -0.003640 -0.006872 -0.000695  0.008530 -0.001112 -0.004956 -0.002562  0.007903 -0.005687 -0.005612  0.008415 -0.007078  0.004249  0.001778  0.005927  0.006101 -0.004065 -0.003867  0.007934  0.000548  0.003470 -0.008821  0.008076\n",
            "dan       -0.000459 -0.008577  0.002718  0.001019  0.007469 -0.002287 -0.002437 -0.000515  0.007667 -0.000800  0.004022 -0.006573 -0.008030 -0.004376 -0.002503  0.008818 -0.002472 -0.001856 -0.000625  0.014263  0.009439  0.000729 -0.003481 -0.008828 -0.012140  0.007784 -0.005681  0.001416 -0.002514 -0.005161  0.001744 -0.003766 -0.007129  0.008848  0.001126  0.011549  0.009207  0.001297  0.008816 -0.001688 -0.007238  0.004278 -0.000064 -0.005806  0.011034 -0.003385  0.005708 -0.000292 -0.006169 -0.000912 -0.002314 -0.000595  0.000596  0.008934  0.006925 -0.005725  0.008019  0.000051 -0.004922 -0.007424 -0.001603 -0.005340  0.007554 -0.004477  0.009045  0.006998  0.000408  0.004346 -0.002324  0.002261  0.002794 -0.010062 -0.008045 -0.001538 -0.000129 -0.003584 -0.001516 -0.003451 -0.001698  0.006543 -0.005885 -0.003726  0.002703  0.005321  0.002703  0.000750  0.007069 -0.006588  0.004901  0.003453  0.001992 -0.006935 -0.004234 -0.005373  0.012430  0.000514 -0.010090 -0.003437 -0.004163  0.003949 -0.013587  0.003985 -0.009061  0.000883  0.005618  0.007867 -0.008132 -0.002529 -0.003214 -0.010117 -0.004262 -0.004923 -0.000976 -0.005389  0.003688  0.001736  0.005324  0.002335  0.002436 -0.004637 -0.003903 -0.000981 -0.006121  0.002344 -0.000568 -0.004816  0.005299  0.007252\n",
            "piala      0.002538  0.003721  0.004926 -0.004023 -0.002409 -0.006233 -0.005317  0.005304 -0.001253 -0.003677 -0.002078  0.003917 -0.003570 -0.004783  0.003809  0.001100 -0.002977  0.001269 -0.004689  0.010411  0.004550 -0.006720  0.002361 -0.008192 -0.002169  0.004841 -0.006507 -0.000893  0.006100 -0.003928 -0.006991  0.007466 -0.001618 -0.002539  0.005851  0.005554  0.005623  0.006745 -0.006561  0.004958  0.000954  0.003718 -0.000090  0.002651  0.005218  0.004546 -0.003764 -0.004629 -0.000784  0.002537 -0.000599  0.005417  0.003314  0.006847  0.002071  0.006261  0.001413  0.005719  0.001397 -0.006739 -0.004266 -0.001365  0.005132 -0.000447  0.000288  0.006240  0.007716 -0.000807 -0.004582 -0.005608 -0.003751 -0.008797 -0.001859 -0.005549  0.002269  0.003344 -0.003757 -0.009086  0.000286 -0.003975  0.000873 -0.003908  0.006700  0.003157  0.004326  0.000359  0.000392  0.003679  0.003051 -0.003056 -0.007346  0.000103  0.005665  0.003475 -0.004543  0.007946  0.001850  0.004286  0.007712 -0.006613 -0.009700  0.005764  0.006902  0.002830  0.006078  0.000092  0.005286 -0.006603 -0.005417 -0.006988 -0.001432  0.002457 -0.003147 -0.006073  0.005436 -0.002188  0.006815 -0.003171  0.003771  0.004539  0.001173 -0.004192  0.001331 -0.005922 -0.003546 -0.003421  0.004317 -0.001944\n",
            "kpk       -0.004871  0.000913 -0.000154 -0.001909  0.007887  0.003024 -0.002667 -0.001341  0.008040  0.008096  0.006580  0.003494 -0.000088 -0.003947 -0.001993  0.004079 -0.003771 -0.000460 -0.008261  0.010212 -0.003182 -0.003920 -0.005663 -0.005399 -0.010036  0.002207 -0.006922  0.000149 -0.006664  0.000678  0.001594 -0.000142 -0.003989 -0.000024  0.004592  0.006232 -0.004156 -0.003348  0.002985  0.003122 -0.009018  0.004202 -0.008718 -0.007230  0.005178 -0.003161  0.000928  0.006384 -0.003552  0.007573  0.003830  0.009149  0.004464 -0.006134 -0.003060  0.004871  0.008214  0.005406  0.003343  0.006242 -0.009785  0.004093 -0.002334  0.003001  0.008184  0.003056  0.006566  0.001814 -0.001585  0.001882 -0.004458 -0.002103  0.002991 -0.007345  0.002959 -0.008530  0.007646  0.004977 -0.000146  0.006343 -0.000541 -0.005755  0.001152  0.008992 -0.001771  0.000350  0.006480 -0.001388  0.000940  0.000049  0.002851 -0.005394 -0.001650 -0.006473  0.000338 -0.005514 -0.005572 -0.009028  0.000752 -0.007764 -0.012244 -0.005157 -0.006716  0.005330  0.003827  0.001485 -0.003678 -0.004457  0.001550 -0.001947  0.002600 -0.001540  0.000218  0.000007 -0.007753 -0.003817 -0.006045  0.000046  0.002676  0.006600 -0.002802 -0.004759  0.006402  0.000959 -0.003725  0.004661  0.000370 -0.003612\n",
            "ke         0.005543  0.001323 -0.005423  0.000701  0.002735  0.004202 -0.000997 -0.003678  0.002138  0.002746  0.002185 -0.002075 -0.006427  0.003627  0.008797  0.010645 -0.009044  0.002571 -0.005921  0.011680  0.004035  0.005233 -0.001273  0.001425 -0.007925  0.007782 -0.004559 -0.002626 -0.002266  0.006709 -0.006341 -0.001627  0.006002  0.001118 -0.006180 -0.002271  0.009454  0.004287  0.004271 -0.001617 -0.008382  0.003407  0.003706 -0.005499 -0.004181 -0.002460 -0.005441 -0.002382 -0.002473 -0.003546  0.009811 -0.006223  0.003545  0.005064 -0.001414  0.006766  0.011161 -0.005418 -0.006646 -0.005619 -0.008007 -0.001678 -0.002971 -0.006561  0.003833  0.002121  0.005310 -0.001525 -0.003865 -0.001392  0.000136  0.004754 -0.003809  0.002889 -0.006274  0.004797 -0.001784  0.005545 -0.007454  0.003299 -0.000750 -0.000559 -0.002048 -0.004121 -0.003942 -0.003328  0.003940 -0.002548  0.000866  0.007778 -0.003742 -0.002025 -0.006358 -0.008955  0.010437 -0.003629 -0.006482  0.002054  0.005044 -0.006208 -0.003033 -0.003511  0.000250 -0.003269 -0.002039  0.002825  0.005601  0.006777 -0.006405 -0.006718  0.008410 -0.000435 -0.003564  0.000756  0.006323 -0.000075 -0.006976  0.002005  0.006251  0.005829  0.002227 -0.007337 -0.001476  0.000815  0.001414 -0.008188 -0.007984  0.000828\n",
            "--------------------------------------------------\n",
            "\n",
            ">>> Bagian 2: Analisis Sentimen (Fallback Kamus, tanpa PyTorch)\n",
            "---\n",
            "Judul 1: 'Jadwal Persib Bandung Hari Ini: Berjuang Bangkit Vs Persebaya'\n",
            "    Label: POSITIVE -> Skor: 1.0000\n",
            "------------------------------\n",
            "Judul 2: 'Persib Bandung Pinjamkan Dimas Drajad ke Malut United'\n",
            "    Label: NEUTRAL -> Skor: 0.0000\n",
            "------------------------------\n",
            "Judul 3: 'Prediksi Persib Bandung vs Persebaya pada Pekan Kelima Super League'\n",
            "    Label: NEUTRAL -> Skor: 0.0000\n",
            "------------------------------\n",
            "Judul 4: 'Persija Jakarta Tunjuk Bambang Pamungkas sebagai Direktur Olahraga'\n",
            "    Label: NEUTRAL -> Skor: 0.0000\n",
            "------------------------------\n",
            "Judul 5: 'Hasil Super League: Persita Raih Kemenangan Perdana, Kalahkan PSM Makassar secara Dramatis'\n",
            "    Label: POSITIVE -> Skor: 0.8000\n",
            "------------------------------\n",
            "Judul 6: 'Permalukan Semen Padang di Kandangnya, Pelatih PSBS Biak Ungkap Kuncinya'\n",
            "    Label: POSITIVE -> Skor: 1.0000\n",
            "------------------------------\n",
            "Judul 7: 'Jurnalis dan Konten Kreator Dibatasi dalam Merekam Pertandingan Sepak Bola'\n",
            "    Label: NEGATIVE -> Skor: 1.0000\n",
            "------------------------------\n",
            "Judul 8: 'Bambang Pamungkas Jadi Direktur Olahraga Persija Jakarta, Apa Tugasnya?'\n",
            "    Label: NEUTRAL -> Skor: 0.0000\n",
            "------------------------------\n",
            "Judul 9: 'Analisis Rahmad Darmawan Soal Kans Timnas Indonesia Lolos Piala Dunia 2026'\n",
            "    Label: POSITIVE -> Skor: 1.0000\n",
            "------------------------------\n",
            "Judul 10: 'Hasil Super League: Semen Padang FC vs PSBS Biak, Skor Akhir 1-2'\n",
            "    Label: NEUTRAL -> Skor: 0.0000\n",
            "------------------------------\n",
            "\n",
            ">>> Bagian 3: Deteksi Entitas Bernama (Menggunakan spaCy)\n",
            "---\n",
            "Teks: Jadwal Persib Bandung Hari Ini: Berjuang Bangkit Vs Persebaya...\n",
            "\n",
            "Entitas yang Terdeteksi:\n",
            "    - Jadwal Persib Bandung Hari Ini (PERSON)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "import spacy\n",
        "from transformers import pipeline\n",
        "\n",
        "# --- Bagian 1: Word Embedding (Pembuatan Representasi Vektor) dengan Gensim ---\n",
        "\n",
        "# Konfigurasi Pandas dan Numpy untuk tampilan penuh\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "print(\">>> Bagian 1: Membuat Word Embedding dengan Gensim\")\n",
        "print(\"---\")\n",
        "\n",
        "# Membaca data\n",
        "try:\n",
        "    df = pd.read_csv('berita_tempo.csv')\n",
        "    print(\"Berhasil memuat seluruh data dari 'berita_tempo.csv'.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File 'berita_tempo.csv' tidak ditemukan.\")\n",
        "    print(\"Gunakan data dummy untuk demonstrasi.\")\n",
        "    data = {'judul': [\n",
        "        \"Manajemen strategi perusahaan menghadapi persaingan global\",\n",
        "        \"Pengaruh kepemimpinan terhadap kinerja organisasi\",\n",
        "        \"Analisis keuangan perusahaan berbasis rasio likuiditas\"\n",
        "    ]}\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "# Gunakan kolom 'judul' sebagai korpus (karena 'isi' NaN semua)\n",
        "corpus = df['judul'].dropna().astype(str).tolist()\n",
        "\n",
        "# Pembersihan teks dasar\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Tokenisasi untuk Word2Vec (list of lists)\n",
        "tokenized_corpus = [clean_text(text).split() for text in corpus if text.strip()]\n",
        "\n",
        "# Bangun model Word2Vec\n",
        "embedding_dim = 128\n",
        "if tokenized_corpus and any(tokenized_corpus):\n",
        "    word2vec_model = Word2Vec(\n",
        "        sentences=tokenized_corpus,\n",
        "        vector_size=embedding_dim,\n",
        "        window=5,\n",
        "        min_count=1,\n",
        "        sg=0  # CBOW, pakai sg=1 kalau mau Skip-gram\n",
        "    )\n",
        "    word2vec_model.train(tokenized_corpus, total_examples=word2vec_model.corpus_count, epochs=10)\n",
        "\n",
        "    words = list(word2vec_model.wv.index_to_key)\n",
        "    vectors = [word2vec_model.wv[word] for word in words]\n",
        "\n",
        "    embedding_df = pd.DataFrame(vectors, index=words)\n",
        "    print(\"\\n>>> Word Embeddings yang sudah terlatih (contoh beberapa kata):\")\n",
        "    print(embedding_df.head(10).to_string())\n",
        "else:\n",
        "    print(\"\\nKorpus kosong atau tidak valid, tidak dapat membuat embeddings.\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"\\n>>> Bagian 2: Analisis Sentimen (Fallback Kamus, tanpa PyTorch)\")\n",
        "print(\"---\")\n",
        "\n",
        "# Kamus kata positif dan negatif (lebih kaya, khusus untuk berita olahraga)\n",
        "positive_words = {\n",
        "    \"bagus\",\"baik\",\"positif\",\"hebat\",\"menang\",\"sukses\",\"senang\",\"gembira\",\"puas\",\n",
        "    \"mantap\",\"unggul\",\"kemenangan\",\"dramatis\",\"permalukan\",\"raih\",\"lolos\",\"ungguli\",\n",
        "    \"bangkit\",\"berhasil\",\"gemilang\",\"pecah\",\"unggulan\"\n",
        "}\n",
        "\n",
        "negative_words = {\n",
        "    \"buruk\",\"jelek\",\"negatif\",\"kalah\",\"gagal\",\"marah\",\"sedih\",\"kecewa\",\"khawatir\",\n",
        "    \"rusak\",\"problem\",\"kontroversi\",\"dibatasi\",\"skandal\",\"batal\",\"terpuruk\",\n",
        "    \"ancaman\",\"hancur\",\"bentrok\",\"krisis\"\n",
        "}\n",
        "\n",
        "def simple_sentiment(text):\n",
        "    text = text.lower()\n",
        "    pos_count = sum(word in text for word in positive_words)\n",
        "    neg_count = sum(word in text for word in negative_words)\n",
        "\n",
        "    if pos_count > neg_count:\n",
        "        return \"POSITIVE\", pos_count / (pos_count + neg_count)\n",
        "    elif neg_count > pos_count:\n",
        "        return \"NEGATIVE\", neg_count / (pos_count + neg_count)\n",
        "    else:\n",
        "        return \"NEUTRAL\", 0.0\n",
        "\n",
        "# Ambil contoh 10 judul pertama\n",
        "judul_contoh = df['judul'].dropna().head(10).tolist()\n",
        "\n",
        "for i, teks in enumerate(judul_contoh, start=1):\n",
        "    label, score = simple_sentiment(teks)\n",
        "    print(f\"Judul {i}: '{teks}'\")\n",
        "    print(f\"    Label: {label} -> Skor: {score:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# --- Bagian 3: Deteksi Entitas Bernama (NER dengan spaCy) ---\n",
        "\n",
        "print(\"\\n>>> Bagian 3: Deteksi Entitas Bernama (Menggunakan spaCy)\")\n",
        "print(\"---\")\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")  # model default bahasa Inggris\n",
        "    # Ambil contoh teks dari dataset (judul pertama)\n",
        "    teks_ner = df['judul'].dropna().astype(str).iloc[0] if not df['judul'].dropna().empty else \\\n",
        "               \"Apple Inc. founded by Steve Jobs in California, is a major technology company.\"\n",
        "\n",
        "    doc = nlp(teks_ner)\n",
        "\n",
        "    print(f\"Teks: {teks_ner[:120]}...\\n\")\n",
        "    print(\"Entitas yang Terdeteksi:\")\n",
        "    for ent in doc.ents:\n",
        "        print(f\"    - {ent.text} ({ent.label_})\")\n",
        "\n",
        "except OSError:\n",
        "    print(\"Model spaCy 'en_core_web_sm' tidak ditemukan.\")\n",
        "    print(\"Silakan install dengan perintah: python -m spacy download en_core_web_sm\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdfe663d",
      "metadata": {
        "id": "fdfe663d"
      },
      "source": [
        "## Klasifikasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a52cc7f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a52cc7f9",
        "outputId": "a0bd4bf5-eb0c-4c95-c52a-3b1738781767"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Berhasil memuat seluruh data dari file CSV: berita_tempo.csv\n",
            "Kolom 'kategori' ditemukan. Menggunakan label dari file.\n",
            "Menggunakan label: {'bola': 0, 'metro': 1, 'dunia': 2}\n",
            "\n",
            "Jumlah total data: 149\n",
            "Jumlah data latih: 119\n",
            "Jumlah data uji: 30\n",
            "\n",
            "==================================================\n",
            ">>> KLASIFIKASI MENGGUNAKAN TF-IDF\n",
            "==================================================\n",
            "\n",
            "--- REPRESENTASI DATA DAN KONTEKS TF-IDF ---\n",
            "           Skor TF-IDF\n",
            "ketua         0.373209\n",
            "perempuan     0.373209\n",
            "sementara     0.373209\n",
            "ma            0.343505\n",
            "ajukan        0.322429\n",
            "eks           0.281432\n",
            "gen           0.281432\n",
            "pm            0.281432\n",
            "jadi          0.271650\n",
            "nepal         0.220870\n",
            "\n",
            "Akurasi TF-IDF: 0.87\n",
            "Laporan Klasifikasi TF-IDF:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86        10\n",
            "           1       1.00      0.90      0.95        10\n",
            "           2       0.80      0.80      0.80        10\n",
            "\n",
            "    accuracy                           0.87        30\n",
            "   macro avg       0.87      0.87      0.87        30\n",
            "weighted avg       0.87      0.87      0.87        30\n",
            "\n",
            "\n",
            "==================================================\n",
            ">>> KLASIFIKASI MENGGUNAKAN WORD EMBEDDING\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Raihan Fadillah\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.3613 - loss: 1.0948\n",
            "Epoch 2/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8992 - loss: 1.0070\n",
            "Epoch 3/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.9073\n",
            "Epoch 4/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.7764\n",
            "Epoch 5/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.6094\n",
            "Epoch 6/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.4250\n",
            "Epoch 7/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.2576\n",
            "Epoch 8/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.1365\n",
            "Epoch 9/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0668\n",
            "Epoch 10/10\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0325\n",
            "Kata 'liverpool' tidak ditemukan di korpus.\n",
            "\n",
            "Akurasi Word Embedding: 0.70\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "Laporan Klasifikasi Word Embedding:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.80      0.89        10\n",
            "           1       0.71      0.50      0.59        10\n",
            "           2       0.53      0.80      0.64        10\n",
            "\n",
            "    accuracy                           0.70        30\n",
            "   macro avg       0.75      0.70      0.71        30\n",
            "weighted avg       0.75      0.70      0.71        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "import sys\n",
        "\n",
        "# --- Fungsi Pembersihan Teks\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# --- Memuat Data dari File CSV\n",
        "file_path = 'berita_tempo.csv'\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Berhasil memuat seluruh data dari file CSV: {file_path}\")\n",
        "\n",
        "    # Periksa apakah kolom 'judul' ada\n",
        "    if 'judul' not in df.columns:\n",
        "        print(\"Error: Kolom 'judul' tidak ditemukan dalam file CSV.\")\n",
        "        sys.exit()\n",
        "\n",
        "    # Periksa apakah kolom 'kategori' ada\n",
        "    if 'kategori' not in df.columns:\n",
        "        print(\"Error: Kolom 'kategori' tidak ditemukan.\")\n",
        "        sys.exit()\n",
        "\n",
        "    # --- Filter data valid ---\n",
        "    df.dropna(subset=['judul'], inplace=True)\n",
        "    counts = df['kategori'].value_counts()\n",
        "    valid_labels = counts[counts >= 2].index\n",
        "    df = df[df['kategori'].isin(valid_labels)]\n",
        "\n",
        "    corpus = df['judul'].tolist()\n",
        "\n",
        "    # Label kategori -> integer\n",
        "    label_to_id = {label: i for i, label in enumerate(df['kategori'].unique())}\n",
        "    labels = [label_to_id[label] for label in df['kategori']]\n",
        "\n",
        "    print(\"Kolom 'kategori' ditemukan. Menggunakan label dari file.\")\n",
        "    print(f\"Menggunakan label: {label_to_id}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File '{file_path}' tidak ditemukan.\")\n",
        "    sys.exit()\n",
        "\n",
        "# --- Bersihkan teks\n",
        "cleaned_corpus = [clean_text(text) for text in corpus]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    cleaned_corpus, labels, test_size=0.2,\n",
        "    random_state=100, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\\nJumlah total data: {len(df)}\")\n",
        "print(f\"Jumlah data latih: {len(X_train)}\")\n",
        "print(f\"Jumlah data uji: {len(X_test)}\")\n",
        "\n",
        "# ======================================================\n",
        "# 1. KLASIFIKASI MENGGUNAKAN TF-IDF\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\">>> KLASIFIKASI MENGGUNAKAN TF-IDF\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Contoh representasi\n",
        "print(\"\\n--- REPRESENTASI DATA DAN KONTEKS TF-IDF ---\")\n",
        "contoh_tfidf = vectorizer.transform([X_train[0]])\n",
        "fitur_tfidf = vectorizer.get_feature_names_out()\n",
        "df_contoh_tfidf = pd.DataFrame(contoh_tfidf.T.todense(), index=fitur_tfidf, columns=['Skor TF-IDF'])\n",
        "df_contoh_tfidf = df_contoh_tfidf[df_contoh_tfidf['Skor TF-IDF'] > 0].sort_values(by='Skor TF-IDF', ascending=False)\n",
        "print(df_contoh_tfidf.head(20).to_string())\n",
        "\n",
        "# Logistic Regression\n",
        "model_tfidf = LogisticRegression(max_iter=1000, C=0.3, class_weight='balanced')\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
        "print(f\"\\nAkurasi TF-IDF: {accuracy_score(y_test, y_pred_tfidf):.2f}\")\n",
        "print(\"Laporan Klasifikasi TF-IDF:\")\n",
        "print(classification_report(y_test, y_pred_tfidf, zero_division=0))\n",
        "\n",
        "# ======================================================\n",
        "# 2. KLASIFIKASI MENGGUNAKAN WORD EMBEDDING (Keras)\n",
        "# ======================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\">>> KLASIFIKASI MENGGUNAKAN WORD EMBEDDING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Tokenisasi dan padding\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_len = max(len(s.split()) for s in cleaned_corpus)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "# Build model\n",
        "model_emb = Sequential()\n",
        "model_emb.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, name=\"embedding_layer\"))\n",
        "model_emb.add(Flatten())\n",
        "model_emb.add(Dense(128, activation='relu'))\n",
        "model_emb.add(Dense(len(set(labels)), activation='softmax'))\n",
        "\n",
        "model_emb.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "model_emb.fit(X_train_padded, np.array(y_train), epochs=10, verbose=1)\n",
        "\n",
        "# Ambil vektor embedding\n",
        "embedding_layer = model_emb.get_layer('embedding_layer')\n",
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "word_index = tokenizer.word_index\n",
        "word_to_vec = {word: embedding_weights[i] for word, i in word_index.items()}\n",
        "\n",
        "if 'liverpool' in word_to_vec:\n",
        "    print(f\"Vektor untuk 'liverpool':\\n{word_to_vec['liverpool'][:10]}...\")\n",
        "else:\n",
        "    print(\"Kata 'liverpool' tidak ditemukan di korpus.\")\n",
        "\n",
        "# Evaluasi\n",
        "loss, accuracy = model_emb.evaluate(X_test_padded, np.array(y_test), verbose=0)\n",
        "print(f\"\\nAkurasi Word Embedding: {accuracy:.2f}\")\n",
        "\n",
        "y_pred_probs_emb = model_emb.predict(X_test_padded)\n",
        "y_pred_emb = np.argmax(y_pred_probs_emb, axis=1)\n",
        "\n",
        "print(\"Laporan Klasifikasi Word Embedding:\")\n",
        "print(classification_report(y_test, y_pred_emb, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2dafaf",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
